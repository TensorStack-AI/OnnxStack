{
  "Logging": {
    "LogLevel": {
      "Default": "Information",
      "Microsoft.AspNetCore": "Warning"
    }
  },
  "AllowedHosts": "*",
  "OnnxStackUIConfig": {
    "ModelCacheMode": "Single",
    "ImageAutoSave": false,
    "ImageAutoSaveBlueprint": false,
    "ImageAutoSaveDirectory": "",
    "RealtimeRefreshRate": 50,
    "RealtimeHistoryEnabled": false,
    "DefaultDeviceId": 0,
    "DefaultInterOpNumThreads": 0,
    "DefaultIntraOpNumThreads": 0,
    "DefaultExecutionMode": "ORT_SEQUENTIAL",
    "DefaultExecutionProvider": "DirectML",

    "Templates": [
      {
        "Name": "StableDiffusion",
        "ImageIcon": "Logo.png",
        "Description": "The stable-diffusion model is a text-to-image diffusion model that can generate highly realistic images based on textual input. It utilizes latent variables and diffusion processes to achieve this.",
        "Rank": 100,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "SD",
        "StableDiffusionTemplate": {
          "ModelType": "Base",
          "SampleSize": 512,
          "PipelineType": "StableDiffusion",
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "SchedulerType": "EulerAncestral"
          }
        }
      },
      {
        "Name": "StableDiffusion Inpaint",
        "ImageIcon": "Logo.png",
        "Description": "Stable Diffusion Inpainting is an image-to-image model that fills in masked parts of images using a stable diffusion process. It is particularly useful when dealing with large missing regions or complex structures. The model uses a patch-based approach to synthesize the missing parts, while ensuring that the inpainted regions are visually consistent with the rest of the image.",
        "Rank": 0,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "SD-Inpaint",
        "StableDiffusionTemplate": {
          "SampleSize": 512,
          "ModelType": "Base",
          "PipelineType": "StableDiffusion",
          "DiffuserTypes": [
            "ImageInpaint"
          ],
          "SchedulerDefaults": {
            "SchedulerType": "EulerAncestral"
          }
        }
      },
      {
        "Name": "StableDiffusion XL",
        "ImageIcon": "Logo.png",
        "Description": "The SDXL model is a text-to-image generative AI model that can create high-resolution images with a size of 1024x1024 pixels. It takes text descriptions as input and generates corresponding images as output. The model aims to create beautiful and realistic images based on the provided textual prompts.",
        "Rank": 99,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "SDXL",
        "StableDiffusionTemplate": {
          "SampleSize": 1024,
          "ModelType": "Base",
          "PipelineType": "StableDiffusionXL",
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "Guidance": 5,
            "SchedulerType": "EulerAncestral"
          }
        }
      },
      {
        "Name": "StableDiffusion XL Inpaint",
        "ImageIcon": "Logo.png",
        "Description": "SDXL Inpainting is an image-to-image model that fills in masked parts of images using a stable diffusion process. It is particularly useful when dealing with large missing regions or complex structures. The model uses a patch-based approach to synthesize the missing parts, while ensuring that the inpainted regions are visually consistent with the rest of the image.",
        "Rank": 0,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "SDXL-Inpaint",
        "StableDiffusionTemplate": {
          "SampleSize": 1024,
          "ModelType": "Base",
          "PipelineType": "StableDiffusionXL",
          "DiffuserTypes": [
            "ImageInpaint"
          ],
          "SchedulerDefaults": {
            "Guidance": 5,
            "SchedulerType": "EulerAncestral"
          }
        }
      },
      {
        "Name": "StableDiffusion XL Refiner",
        "ImageIcon": "Logo.png",
        "Description": "SDXL consists of an ensemble of experts pipeline for latent diffusion: In a first step, the base model is used to generate (noisy) latents, which are then further processed with a refinement model specialized for the final denoising steps. Note that this model can not be used as a standalone module.",
        "Rank": 0,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "SDXL-Refiner",
        "StableDiffusionTemplate": {
          "SampleSize": 1024,
          "ModelType": "Refiner",
          "PipelineType": "StableDiffusionXL",
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "Guidance": 5,
            "SchedulerType": "EulerAncestral"
          }
        }
      },
      {
        "Name": "StableDiffusion XL Turbo",
        "ImageIcon": "Logo.png",
        "Description": "SDXL-Turbo is a distilled version of SDXL 1.0, trained for real-time synthesis. SDXL-Turbo is based on a novel training method called Adversarial Diffusion Distillation (ADD) (see the technical report), which allows sampling large-scale foundational image diffusion models in 1 to 4 steps at high image quality. This approach uses score distillation to leverage large-scale off-the-shelf image diffusion models as a teacher signal and combines this with an adversarial loss to ensure high image fidelity even in the low-step regime of one or two sampling steps.",
        "Rank": 0,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "SDXL-Turbo",
        "StableDiffusionTemplate": {
          "SampleSize": 512,
          "ModelType": "Base",
          "PipelineType": "StableDiffusionXL",
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "Steps": 4,
            "StepsMin": 1,
            "Guidance": 0,
            "GuidanceMin": 0,
            "GuidanceMax": 0,
            "SchedulerType": "EulerAncestral"
          }
        }
      },
      {
        "Name": "Latent Consistency",
        "ImageIcon": "Logo.png",
        "Description": "Latent Consistency Model (LCM) was proposed in Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference by Simian Luo, Yiqin Tan et al. and Simian Luo, Suraj Patil, and Daniel Gu succesfully applied the same approach to create LCM for SDXL.",
        "Rank": 90,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "LCM",
        "StableDiffusionTemplate": {
          "SampleSize": 512,
          "ModelType": "Base",
          "PipelineType": "LatentConsistency",
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "Steps": 6,
            "Guidance": 1,
            "GuidanceMin": 0,
            "GuidanceMax": 2,
            "SchedulerType": "LCM"
          }
        }
      },
      {
        "Name": "Latent Consistency XL",
        "ImageIcon": "Logo.png",
        "Description": "Latent Consistency Model (LCM) was proposed in Latent Consistency Models: Synthesizing High-Resolution Images with Few-Step Inference by Simian Luo, Yiqin Tan et al. and Simian Luo, Suraj Patil, and Daniel Gu succesfully applied the same approach to create LCM for SDXL.\nThis checkpoint is a LCM distilled version of stable-diffusion-xl-base-1.0 that allows to reduce the number of inference steps to only between 2 - 8 steps.",
        "Rank": 90,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "LCM-SDXL",
        "StableDiffusionTemplate": {
          "SampleSize": 1024,
          "ModelType": "Base",
          "PipelineType": "LatentConsistencyXL",
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "Steps": 6,
            "Guidance": 1,
            "GuidanceMin": 0,
            "GuidanceMax": 2,
            "SchedulerType": "LCM"
          }
        }
      },
      {
        "Name": "InstaFlow",
        "ImageIcon": "",
        "Description": "InstaFlow is an ultra-fast, one-step image generator that achieves image quality close to Stable Diffusion, significantly reducing the demand of computational resources. This efficiency is made possible through a recent Rectified Flow technique, which trains probability flows with straight trajectories, hence inherently requiring only a single step for fast inference.",
        "Rank": 0,
        "Author": "OnnxStack",
        "Category": "StableDiffusion",
        "Template": "InstaFlow",
        "StableDiffusionTemplate": {
          "SampleSize": 512,
          "ModelType": "Base",
          "PipelineType": "InstaFlow",
          "DiffuserTypes": [
            "TextToImage"
          ],
          "SchedulerDefaults": {
            "Steps": 1,
            "StepsMin": 1,
            "StepsMax": 1,
            "Guidance": 0,
            "GuidanceMin": 0,
            "GuidanceMax": 0,
            "SchedulerType": "InstaFlow"
          }
        }
      },
      {
        "Name": "Upscaler x2",
        "ImageIcon": "",
        "Description": "Enhance your images with double the detail using our 2x upscaling feature. Elevate the clarity and quality of your photos with just a tap, bringing out finer details and making your visuals stand out effortlessly",
        "Author": "OnnxStack",
        "Category": "Upscaler",
        "Template": "Upscaler",
        "UpscaleTemplate": {
          "Channels": 3,
          "ScaleFactor": 2,
          "SampleSize": 512
        }
      },
      {
        "Name": "Upscaler x4",
        "ImageIcon": "",
        "Description": "Experience a new level of image perfection with our 4x upscaling feature. Transform your photos with quadruple the resolution, capturing every nuance and detail. Elevate your visual content to new heights, ensuring breathtaking clarity and sharpness in every image.",
        "Author": "OnnxStack",
        "Category": "Upscaler",
        "Template": "Upscaler",
        "UpscaleTemplate": {
          "Channels": 3,
          "ScaleFactor": 4,
          "SampleSize": 512
        }
      },
      {
        "Name": "Stable Diffusion 1.5",
        "ImageIcon": "https://www.runwayml.com/images/logo-square.png",
        "Author": "RunwayML",
        "Website": "https://runwayml.com/",
        "Description": "Stable Diffusion is a text-to-image latent diffusion model created by the researchers and engineers from CompVis, Stability AI and LAION. It is trained on 512x512 images from a subset of the LAION-5B database. LAION-5B is the largest, freely accessible multi-modal dataset that currently exists.",
        "Tags": [ "CPU", "GPU", "F32" ],
        "IsUserTemplate": true,
        "Template": "SD",
        "Category": "StableDiffusion",
        "StableDiffusionTemplate": {
          "PipelineType": "StableDiffusion",
          "ModelType": "Base",
          "SampleSize": 512,
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "SchedulerType": "EulerAncestral"
          }
        },
        "Repository": "https://huggingface.co/runwayml/stable-diffusion-v1-5",
        "RepositoryBranch": "onnx",
        "RepositoryFiles": [
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/unet/model.onnx",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/unet/weights.pb",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/text_encoder/model.onnx",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/vae_decoder/model.onnx",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/vae_encoder/model.onnx"
        ],
        "PreviewImages": [
          "https://raw.githubusercontent.com/saddam213/OnnxStack/master/Assets/Templates/stable-diffusion-v1-5/Preview1.png",
          "https://raw.githubusercontent.com/saddam213/OnnxStack/master/Assets/Templates/stable-diffusion-v1-5/Preview2.png",
          "https://raw.githubusercontent.com/saddam213/OnnxStack/master/Assets/Templates/stable-diffusion-v1-5/Preview3.png",
          "https://raw.githubusercontent.com/saddam213/OnnxStack/master/Assets/Templates/stable-diffusion-v1-5/Preview4.png"
        ]
      },
      {
        "Name": "Stable Diffusion XL",
        "ImageIcon": "https://cdn-uploads.huggingface.co/production/uploads/643feeb67bc3fbde1385cc25/7vmYr2XwVcPtkLzac_jxQ.png",
        "Author": "Stability AI",
        "Website": "https://platform.stability.ai",
        "Description": "SDXL consists of an ensemble of experts pipeline for latent diffusion: In a first step, the base model is used to generate (noisy) latents, which are then further processed with a refinement model (available here: https://huggingface.co/stabilityai/stable-diffusion-xl-refiner-1.0/) specialized for the final denoising steps. Note that the base model can be used as a standalone module.",
        "Tags": [ "CPU", "GPU", "F32" ],
        "IsUserTemplate": true,
        "Template": "SDXL",
        "Category": "StableDiffusion",
        "StableDiffusionTemplate": {
          "PipelineType": "StableDiffusionXL",
          "ModelType": "Base",
          "SampleSize": 1024,
          "DiffuserTypes": [
            "TextToImage",
            "ImageToImage",
            "ImageInpaintLegacy"
          ],
          "SchedulerDefaults": {
            "Guidance": 5,
            "SchedulerType": "EulerAncestral"
          }
        },
        "Repository": "https://huggingface.co/stabilityai/stable-diffusion-xl-base-1.0",
        "RepositoryFiles": [
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/unet/model.onnx",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/unet/model.onnx_data",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/text_encoder/model.onnx",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/text_encoder_2/model.onnx",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/text_encoder_2/model.onnx_data",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/vae_decoder/model.onnx",
          "https://huggingface.co/runwayml/stable-diffusion-v1-5/resolve/onnx/vae_encoder/model.onnx"
        ],
        "PreviewImages": [
        ]
      },
      {
        "Name": "SwinIR-x4",
        "ImageIcon": "",
        "Author": "rocca",
        "Description": "SwinIR, a robust image restoration model based on the Swin Transformer, outperforms existing methods in tasks like image super-resolution, denoising, and JPEG compression artifact reduction. The model's three components—shallow feature extraction, deep feature extraction using residual Swin Transformer blocks, and high-quality image reconstruction—yield superior results with up to 0.45dB improvement across tasks. Notably, SwinIR achieves this while reducing the total number of parameters by up to 67%.",
        "Website": "https://arxiv.org/abs/2108.10257",
        "Tags": [ "CPU", "GPU", "F32" ],
        "IsUserTemplate": true,
        "Category": "Upscaler",
        "Template": "Upscaler",
        "UpscaleTemplate": {
          "Channels": 3,
          "ScaleFactor": 4,
          "SampleSize": 512
        },
        "Repository": "https://huggingface.co/rocca/swin-ir-onnx",
        "RepositoryFiles": [
          "https://huggingface.co/rocca/swin-ir-onnx/resolve/main/003_realSR_BSRGAN_DFO_s64w8_SwinIR-M_x4_GAN.onnx"
        ]
      }
    ]
  }
}